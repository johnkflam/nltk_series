{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.youtube.com/watch?v=imPpT2Qo2sk&t=6s\n",
    "\n",
    "https://pythonprogramming.net/chunking-nltk-tutorial/\n",
    "\n",
    "Chunking - figuring out meaning of the sentences\n",
    "\n",
    "Regular Expression\n",
    "https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifiers:\n",
    "\n",
    "{1,3} = for digits, u expect 1-3 counts of digits, or \"places\"\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\n",
    "$ = matches at the end of string\n",
    "^ = matches start of a string\n",
    "| = matches either/or. Example x|y = will match either x or y\n",
    "[] = range, or \"variance\"\n",
    "{x} = expect to see this amount of the preceding code.\n",
    "{x,y} = expect to see this x-y amounts of the precedng code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAfter seperating a document into words and sentences, what is the next step to get meaning out of it?\\n1) Figure out the Name entity - subject or noun\\n2) Identify verb\\n3) Noun phrase\\n\\nPOS tag list:\\n\\nCC\\tcoordinating conjunction\\nCD\\tcardinal digit\\nDT\\tdeterminer\\nEX\\texistential there (like: \"there is\" ... think of it like \"there exists\")\\nFW\\tforeign word\\nIN\\tpreposition/subordinating conjunction\\nJJ\\tadjective\\t\\'big\\'\\nJJR\\tadjective, comparative\\t\\'bigger\\'\\nJJS\\tadjective, superlative\\t\\'biggest\\'\\nLS\\tlist marker\\t1)\\nMD\\tmodal\\tcould, will\\nNN\\tnoun, singular \\'desk\\'\\nNNS\\tnoun plural\\t\\'desks\\'\\nNNP\\tproper noun, singular\\t\\'Harrison\\'\\nNNPS\\tproper noun, plural\\t\\'Americans\\'\\nPDT\\tpredeterminer\\t\\'all the kids\\'\\nPOS\\tpossessive ending\\tparent\\'s\\nPRP\\tpersonal pronoun\\tI, he, she\\nPRP$\\tpossessive pronoun\\tmy, his, hers\\nRB\\tadverb\\tvery, silently,\\nRBR\\tadverb, comparative\\tbetter\\nRBS\\tadverb, superlative\\tbest\\nRP\\tparticle\\tgive up\\nTO\\tto\\tgo \\'to\\' the store.\\nUH\\tinterjection\\terrrrrrrrm\\nVB\\tverb, base form\\ttake\\nVBD\\tverb, past tense\\ttook\\nVBG\\tverb, gerund/present participle\\ttaking\\nVBN\\tverb, past participle\\ttaken\\nVBP\\tverb, sing. present, non-3d\\ttake\\nVBZ\\tverb, 3rd person sing. present\\ttakes\\nWDT\\twh-determiner\\twhich\\nWP\\twh-pronoun\\twho, what\\nWP$\\tpossessive wh-pronoun\\twhose\\nWRB\\twh-abverb\\twhere, when\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "After seperating a document into words and sentences, what is the next step to get meaning out of it?\n",
    "1) Figure out the Name entity - subject or noun\n",
    "2) Identify verb\n",
    "3) Noun phrase\n",
    "\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0402097902098 0.0735785953177 0.0383693045564 5720 299 230 22\n",
      "0.000174825174825 0.00334448160535 0.0 5720 299 1 1\n",
      "0.00437062937063 0.00334448160535 0.00442722744881 5720 299 25 1\n",
      "0.000699300699301 0.00334448160535 0.000553403431101 5720 299 4 1\n",
      "0.0013986013986 0.00334448160535 0.00129127467257 5720 299 8 1\n",
      "0.0171328671329 0.0401337792642 0.0158642316916 5720 299 98 12\n",
      "0.00122377622378 0.0066889632107 0.000922339051835 5720 299 7 2\n",
      "0.00384615384615 0.0066889632107 0.00368935620734 5720 299 22 2\n",
      "0.00157342657343 0.00334448160535 0.00147574248294 5720 299 9 1\n",
      "0.000874125874126 0.0066889632107 0.000553403431101 5720 299 5 2\n",
      "0.00314685314685 0.0133779264214 0.00258254934514 5720 299 18 4\n",
      "0.00244755244755 0.0066889632107 0.00221361372441 5720 299 14 2\n",
      "0.00192307692308 0.00334448160535 0.00184467810367 5720 299 11 1\n",
      "0.00157342657343 0.00334448160535 0.00147574248294 5720 299 9 1\n",
      "0.0138111888112 0.0200668896321 0.0134661501568 5720 299 79 6\n",
      "0.00034965034965 0.00334448160535 0.000184467810367 5720 299 2 1\n",
      "0.00227272727273 0.00334448160535 0.00221361372441 5720 299 13 1\n",
      "0.00122377622378 0.0167224080268 0.000368935620734 5720 299 7 5\n",
      "0.00244755244755 0.0167224080268 0.0016602102933 5720 299 14 5\n",
      "0.000174825174825 0.00334448160535 0.0 5720 299 1 1\n",
      "0.000874125874126 0.0066889632107 0.000553403431101 5720 299 5 2\n",
      "0.00996503496503 0.0066889632107 0.0101457295702 5720 299 57 2\n",
      "0.0013986013986 0.00334448160535 0.00129127467257 5720 299 8 1\n",
      "0.000174825174825 0.00334448160535 0.0 5720 299 1 1\n",
      "0.000874125874126 0.0066889632107 0.000553403431101 5720 299 5 2\n",
      "0.000174825174825 0.00334448160535 0.0 5720 299 1 1\n",
      "0.000524475524476 0.00334448160535 0.000368935620734 5720 299 3 1\n",
      "0.00034965034965 0.0066889632107 0.0 5720 299 2 2\n",
      "0.0295454545455 0.0066889632107 0.0308061243313 5720 299 169 2\n",
      "0.0131118881119 0.0100334448161 0.0132816823464 5720 299 75 3\n",
      "0.000174825174825 0.00334448160535 0.0 5720 299 1 1\n",
      "0.00437062937063 0.00334448160535 0.00442722744881 5720 299 25 1\n",
      "0.0162587412587 0.0802675585284 0.0127282789153 5720 299 93 24\n",
      "0.00034965034965 0.00334448160535 0.000184467810367 5720 299 2 1\n",
      "0.000174825174825 0.00334448160535 0.0 5720 299 1 1\n",
      "0.00244755244755 0.00334448160535 0.00239808153477 5720 299 14 1\n",
      "0.00157342657343 0.0066889632107 0.00129127467257 5720 299 9 2\n",
      "0.0013986013986 0.00334448160535 0.00129127467257 5720 299 8 1\n",
      "0.00314685314685 0.00334448160535 0.00313595277624 5720 299 18 1\n",
      "0.000524475524476 0.0066889632107 0.000184467810367 5720 299 3 2\n",
      "0.0034965034965 0.00334448160535 0.00350488839697 5720 299 20 1\n",
      "0.00611888111888 0.00334448160535 0.00627190555248 5720 299 35 1\n",
      "0.00524475524476 0.0066889632107 0.00516509869028 5720 299 30 2\n",
      "0.00104895104895 0.00334448160535 0.000922339051835 5720 299 6 1\n",
      "0.0155594405594 0.00334448160535 0.0162331673123 5720 299 89 1\n",
      "0.00227272727273 0.00334448160535 0.00221361372441 5720 299 13 1\n",
      "0.00297202797203 0.00334448160535 0.00295148496587 5720 299 17 1\n",
      "0.000699300699301 0.00334448160535 0.000553403431101 5720 299 4 1\n",
      "0.00594405594406 0.0267558528428 0.00479616306954 5720 299 34 8\n",
      "0.00034965034965 0.00334448160535 0.000184467810367 5720 299 2 1\n",
      "0.00262237762238 0.0100334448161 0.00221361372441 5720 299 15 3\n",
      "0.00104895104895 0.0133779264214 0.000368935620734 5720 299 6 4\n",
      "0.00314685314685 0.0066889632107 0.00295148496587 5720 299 18 2\n",
      "0.00104895104895 0.00334448160535 0.000922339051835 5720 299 6 1\n",
      "0.000174825174825 0.00334448160535 0.0 5720 299 1 1\n",
      "0.0449300699301 0.0434782608696 0.0450101457296 5720 299 257 13\n"
     ]
    }
   ],
   "source": [
    "# train custom tokenizer and use it to tokenize sample text\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    tokenized -> tagged -> setup chunk as regex -> initialize parser -> parse -> draw\n",
    "'''\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
